{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bebee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55d8f521",
   "metadata": {},
   "outputs": [],
   "source": [
    "nesting_start = 3\n",
    "efficient = True\n",
    "nesting_list = [2**i for i in range(nesting_start, 12)] # 8, 16, 32, 64, 128, 256, 512, 1024, 2048\n",
    "num_classes = 10 # mnist = 10\n",
    "model_path = \"/home/iliam/dl-project/model/final_weights.pt\"  # Update with your actual path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445c2056",
   "metadata": {},
   "source": [
    "## Loading and checkind datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d14ef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "mnist_test = torchvision.datasets.MNIST('./mnist', download=True, train=False, transform=torchvision.transforms.ToTensor())\n",
    "cifar10_test = torchvision.datasets.CIFAR10('./cifar10', download=True, train=False, transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e042d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(mnist_test))\n",
    "input_image, target_class = mnist_test[0]\n",
    "print(target_class)\n",
    "input_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13d923e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(cifar10_test))\n",
    "input_image, target_clasee = cifar10_test[0]\n",
    "print(target_class)\n",
    "input_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa23457",
   "metadata": {},
   "source": [
    "# Applying model + MRL to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6562c866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iliam/MRL-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing resnet50 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iliam/MRL-env/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/iliam/MRL-env/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying BlurPool...\n",
      "Loading weights from /home/iliam/dl-project/model/final_weights.pt...\n",
      "Error loading weights: Error(s) in loading state_dict for ResNet:\n",
      "\tsize mismatch for fc.nesting_classifier_0.weight: copying a param with shape torch.Size([1000, 2048]) from checkpoint, the shape in current model is torch.Size([10, 2048]).\n",
      "\tsize mismatch for fc.nesting_classifier_0.bias: copying a param with shape torch.Size([1000]) from checkpoint, the shape in current model is torch.Size([10]).\n",
      "Continuing with pretrained weights only.\n",
      "Model loaded on cuda\n",
      "Available embedding dimensions: [8, 16, 32, 64, 128, 256, 512, 1024, 2048]\n",
      "Output embedding dimension set to 8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasets import load_from_disk, Dataset\n",
    "from PIL import Image\n",
    "from mrl_loader import MRLLoader\n",
    "\n",
    "model = MRLLoader(weights_path=model_path, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f350093",
   "metadata": {},
   "source": [
    "## Working with MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a60246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_to_tensors(mnist_dataset):\n",
    "    # Get all images and labels\n",
    "    images = torch.stack([img for img, _ in mnist_dataset])\n",
    "    labels = torch.tensor([label for _, label in mnist_dataset])\n",
    "    \n",
    "    # Add channel dimension if needed (MNIST usually has shape [N, H, W])\n",
    "    if images.ndim == 3:\n",
    "        images = images.unsqueeze(1)  # [N, 1, H, W]\n",
    "    \n",
    "    # Convert to 3-channel\n",
    "    images = images.repeat(1, 3, 1, 1)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "mnist_images, mnist_labels = mnist_to_tensors(mnist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "aebb7c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 2, 1,  ..., 4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "print(mnist_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f565c413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dim 8: torch.Size([10000, 8])\n",
      "Embedding dim 16: torch.Size([10000, 16])\n",
      "Embedding dim 32: torch.Size([10000, 32])\n",
      "Embedding dim 64: torch.Size([10000, 64])\n",
      "Embedding dim 128: torch.Size([10000, 128])\n",
      "Embedding dim 256: torch.Size([10000, 256])\n",
      "Embedding dim 512: torch.Size([10000, 512])\n",
      "Embedding dim 1024: torch.Size([10000, 1024])\n",
      "Embedding dim 2048: torch.Size([10000, 2048])\n"
     ]
    }
   ],
   "source": [
    "mnist_embeds = model.extract_all_embeddings(mnist_images)\n",
    "for dim, emb in mnist_embeds.items():\n",
    "    print(f\"Embedding dim {dim}: {emb.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7bfb8843",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def train_evaluate_logreg(embeddings_dict, labels, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Train and evaluate logistic regression classifiers for each embedding dimension.\n",
    "    \n",
    "    Args:\n",
    "        embeddings_dict: Dictionary {dim: embeddings_tensor} where tensors are on CUDA\n",
    "        labels: Tensor of corresponding labels\n",
    "        test_size: Fraction of data to use for validation\n",
    "        random_state: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of {dim: accuracy_score} for each dimension\n",
    "    \"\"\"\n",
    "    # Convert labels to numpy (move to CPU if needed)\n",
    "    labels_np = labels.cpu().numpy() if labels.is_cuda else labels.numpy()\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for dim, embeddings in embeddings_dict.items():\n",
    "        # Convert embeddings to numpy (move to CPU if needed)\n",
    "        emb_np = embeddings.cpu().numpy() if embeddings.is_cuda else embeddings.numpy()\n",
    "        \n",
    "        # Split into train/test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            emb_np, labels_np, \n",
    "            test_size=test_size, \n",
    "            random_state=random_state,\n",
    "            stratify=labels_np  # Preserve class distribution\n",
    "        )\n",
    "        \n",
    "        # Train logistic regression\n",
    "        logreg = LogisticRegression(\n",
    "            max_iter=1000,  # Increased for convergence\n",
    "            solver='lbfgs',  # Good for multiclass\n",
    "            random_state=random_state\n",
    "        )\n",
    "        logreg.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = logreg.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        results[dim] = accuracy\n",
    "        \n",
    "        print(f\"Dimension {dim}: Test Accuracy = {accuracy:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fdd7222b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension 8: Test Accuracy = 0.1465\n",
      "Dimension 16: Test Accuracy = 0.1690\n",
      "Dimension 32: Test Accuracy = 0.2580\n",
      "Dimension 64: Test Accuracy = 0.6055\n",
      "Dimension 128: Test Accuracy = 0.8050\n",
      "Dimension 256: Test Accuracy = 0.8705\n",
      "Dimension 512: Test Accuracy = 0.9180\n",
      "Dimension 1024: Test Accuracy = 0.9380\n",
      "Dimension 2048: Test Accuracy = 0.9400\n"
     ]
    }
   ],
   "source": [
    "results = train_evaluate_logreg(mnist_embeds, mnist_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b915ee0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim 8: Predictions shape torch.Size([10000, 10]), Embedding shape torch.Size([10000, 8])\n",
      "Dim 16: Predictions shape torch.Size([10000, 10]), Embedding shape torch.Size([10000, 16])\n",
      "Dim 32: Predictions shape torch.Size([10000, 10]), Embedding shape torch.Size([10000, 32])\n",
      "Dim 64: Predictions shape torch.Size([10000, 10]), Embedding shape torch.Size([10000, 64])\n",
      "Dim 128: Predictions shape torch.Size([10000, 10]), Embedding shape torch.Size([10000, 128])\n",
      "Dim 256: Predictions shape torch.Size([10000, 10]), Embedding shape torch.Size([10000, 256])\n",
      "Dim 512: Predictions shape torch.Size([10000, 10]), Embedding shape torch.Size([10000, 512])\n",
      "Dim 1024: Predictions shape torch.Size([10000, 10]), Embedding shape torch.Size([10000, 1024])\n",
      "Dim 2048: Predictions shape torch.Size([10000, 10]), Embedding shape torch.Size([10000, 2048])\n"
     ]
    }
   ],
   "source": [
    "all_results = model.predict_with_all_embeddings(mnist_images, return_probabilities=True)\n",
    "for dim, (probs, emb) in all_results.items():\n",
    "    print(f\"Dim {dim}: Predictions shape {probs.shape}, Embedding shape {emb.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db69d414",
   "metadata": {},
   "source": [
    "## Working with cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6dbcc8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar10_to_tensors(cifar10_dataset):\n",
    "    # Get all images and labels\n",
    "    images = torch.stack([img for img, _ in cifar10_dataset])\n",
    "    labels = torch.tensor([label for _, label in cifar10_dataset])\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "cifar10_images, cifar10_labels = cifar10_to_tensors(cifar10_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "64dbecd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "print(cifar10_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cc75cd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dim 8: torch.Size([10000, 8])\n",
      "Embedding dim 16: torch.Size([10000, 16])\n",
      "Embedding dim 32: torch.Size([10000, 32])\n",
      "Embedding dim 64: torch.Size([10000, 64])\n",
      "Embedding dim 128: torch.Size([10000, 128])\n",
      "Embedding dim 256: torch.Size([10000, 256])\n",
      "Embedding dim 512: torch.Size([10000, 512])\n",
      "Embedding dim 1024: torch.Size([10000, 1024])\n",
      "Embedding dim 2048: torch.Size([10000, 2048])\n"
     ]
    }
   ],
   "source": [
    "cifar10_embeds = model.extract_all_embeddings(cifar10_images)\n",
    "for dim, emb in cifar10_embeds.items():\n",
    "    print(f\"Embedding dim {dim}: {emb.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d5538d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension 8: Test Accuracy = 0.1050\n",
      "Dimension 16: Test Accuracy = 0.1150\n",
      "Dimension 32: Test Accuracy = 0.1355\n",
      "Dimension 64: Test Accuracy = 0.2405\n",
      "Dimension 128: Test Accuracy = 0.3735\n",
      "Dimension 256: Test Accuracy = 0.4430\n",
      "Dimension 512: Test Accuracy = 0.4765\n",
      "Dimension 1024: Test Accuracy = 0.4770\n",
      "Dimension 2048: Test Accuracy = 0.4850\n"
     ]
    }
   ],
   "source": [
    "cifar10_results = train_evaluate_logreg(cifar10_embeds, cifar10_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb18932",
   "metadata": {},
   "source": [
    "## Working with CelebFaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28186ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing resnet50 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iliam/MRL-env/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/iliam/MRL-env/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying BlurPool...\n",
      "Loading weights from /home/iliam/dl-project/model/final_weights.pt...\n",
      "Error loading weights: Error(s) in loading state_dict for ResNet:\n",
      "\tsize mismatch for fc.nesting_classifier_0.weight: copying a param with shape torch.Size([1000, 2048]) from checkpoint, the shape in current model is torch.Size([2, 2048]).\n",
      "\tsize mismatch for fc.nesting_classifier_0.bias: copying a param with shape torch.Size([1000]) from checkpoint, the shape in current model is torch.Size([2]).\n",
      "Continuing with pretrained weights only.\n",
      "Model loaded on cuda\n",
      "Available embedding dimensions: [8, 16, 32, 64, 128, 256, 512, 1024, 2048]\n",
      "Output embedding dimension set to 8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "class CelebAAttributesDataset(Dataset):\n",
    "    def __init__(self, img_dir, attr_path, image_size=224):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_dir: Directory with all the images\n",
    "            attr_path: Path to list_attr_celeba.csv\n",
    "            image_size: Target image size\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.image_files = sorted(os.listdir(img_dir))\n",
    "        \n",
    "        # Load and preprocess attributes\n",
    "        self.attributes = pd.read_csv(attr_path)\n",
    "        self.attributes.replace(-1, 0, inplace=True)  # Replace -1 with 0\n",
    "        self.attr_names = self.attributes.columns[1:]  # Skip image_id column\n",
    "        \n",
    "        # Image transformations\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.CenterCrop(178),  # Original celebA is 218x178\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet stats\n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.img_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        # Get corresponding attributes\n",
    "        img_id = self.image_files[idx]\n",
    "        attrs = self.attributes[self.attributes['image_id'] == img_id].iloc[0,1:].values\n",
    "        attrs = torch.tensor(attrs.astype('float32'))\n",
    "        \n",
    "        return image, attrs\n",
    "\n",
    "image_folder = \"/home/iliam/datasets/img_align_celeba/img_align_celeba\"\n",
    "attributes_folder = \"/home/iliam/datasets/list_attr_celeba.csv\"\n",
    "dataset = CelebAAttributesDataset(img_dir=image_folder, attr_path=attributes_folder, image_size=224)\n",
    "model = MRLLoader(weights_path=model_path, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5a9b09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def extract_celeba_embeddings(model, dataset, batch_size=32, device='cuda'):\n",
    "    \"\"\"\n",
    "    Extract embeddings for CelebA images\n",
    "    \n",
    "    Args:\n",
    "        model: Your embedding model\n",
    "        dataset: CelebAAttributesDataset instance\n",
    "        batch_size: Batch size for processing\n",
    "        device: Device to use\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (embeddings, attributes) where:\n",
    "        - embeddings: Dictionary {dim: tensor_of_embeddings}\n",
    "        - attributes: Tensor of all attributes\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize storage\n",
    "    available_dims = sorted(model.get_available_dimensions())\n",
    "    embeddings = {dim: [] for dim in available_dims}\n",
    "    all_attrs = []\n",
    "    \n",
    "    # Create dataloader\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # Process batches\n",
    "    for batch_imgs, batch_attrs in tqdm(dataloader, desc=\"Extracting embeddings\"):\n",
    "        batch_imgs = batch_imgs.to(device)\n",
    "        \n",
    "        batch_embeddings = model.extract_all_embeddings(batch_imgs)\n",
    "            \n",
    "        for dim in available_dims:\n",
    "            embeddings[dim].append(batch_embeddings[dim].cpu())\n",
    "            \n",
    "        all_attrs.append(batch_attrs.cpu())\n",
    "    \n",
    "    # Concatenate results\n",
    "    final_embeddings = {\n",
    "        dim: torch.cat(emb_list, dim=0) \n",
    "        for dim, emb_list in embeddings.items()\n",
    "    }\n",
    "    final_attrs = torch.cat(all_attrs, dim=0)\n",
    "    \n",
    "    return final_embeddings, final_attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10aa2143",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings:  68%|██████▊   | 1071/1583 [08:13<05:12,  1.64it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "embeddings, attributes = extract_celeba_embeddings(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aadc76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MRL-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
