{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ef698dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "\n",
    "# Define the MRL components from the code base\n",
    "class MRL_Linear_Layer(nn.Module):\n",
    "    def __init__(self, nesting_list, num_classes=1000, efficient=False, **kwargs):\n",
    "        super(MRL_Linear_Layer, self).__init__()\n",
    "        self.nesting_list = nesting_list\n",
    "        self.num_classes = num_classes\n",
    "        self.efficient = efficient\n",
    "        \n",
    "        if self.efficient:\n",
    "            setattr(self, f\"nesting_classifier_{0}\", nn.Linear(nesting_list[-1], self.num_classes, **kwargs))\n",
    "        else:    \n",
    "            for i, num_feat in enumerate(self.nesting_list):\n",
    "                setattr(self, f\"nesting_classifier_{i}\", nn.Linear(num_feat, self.num_classes, **kwargs))    \n",
    "\n",
    "    def reset_parameters(self):\n",
    "        if self.efficient:\n",
    "            self.nesting_classifier_0.reset_parameters()\n",
    "        else:\n",
    "            for i in range(len(self.nesting_list)):\n",
    "                getattr(self, f\"nesting_classifier_{i}\").reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        nesting_logits = ()\n",
    "        for i, num_feat in enumerate(self.nesting_list):\n",
    "            if self.efficient:\n",
    "                if self.nesting_classifier_0.bias is None:\n",
    "                    nesting_logits += (torch.matmul(x[:, :num_feat], (self.nesting_classifier_0.weight[:, :num_feat]).t()), )\n",
    "                else:\n",
    "                    nesting_logits += (torch.matmul(x[:, :num_feat], (self.nesting_classifier_0.weight[:, :num_feat]).t()) + self.nesting_classifier_0.bias, )\n",
    "            else:\n",
    "                nesting_logits += (getattr(self, f\"nesting_classifier_{i}\")(x[:, :num_feat]),)\n",
    "\n",
    "        return nesting_logits\n",
    "\n",
    "class FixedFeatureLayer(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, **kwargs):\n",
    "        super(FixedFeatureLayer, self).__init__(in_features, out_features, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not (self.bias is None):\n",
    "            out = torch.matmul(x[:, :self.in_features], self.weight.t()) + self.bias\n",
    "        else:\n",
    "            out = torch.matmul(x[:, :self.in_features], self.weight.t())\n",
    "        return out\n",
    "\n",
    "class BlurPoolConv2d(nn.Module):\n",
    "    def __init__(self, conv):\n",
    "        super().__init__()\n",
    "        default_filter = torch.tensor([[[[1, 2, 1], [2, 4, 2], [1, 2, 1]]]]) / 16.0\n",
    "        filt = default_filter.repeat(conv.in_channels, 1, 1, 1)\n",
    "        self.conv = conv\n",
    "        self.register_buffer('blur_filter', filt)\n",
    "\n",
    "    def forward(self, x):\n",
    "        blurred = F.conv2d(x, self.blur_filter, stride=1, padding=(1, 1),\n",
    "                           groups=self.conv.in_channels, bias=None)\n",
    "        return self.conv.forward(blurred)\n",
    "\n",
    "def apply_blurpool(mod: nn.Module):\n",
    "    for (name, child) in mod.named_children():\n",
    "        if isinstance(child, nn.Conv2d) and (np.max(child.stride) > 1 and child.in_channels >= 16):\n",
    "            setattr(mod, name, BlurPoolConv2d(child))\n",
    "        else: \n",
    "            apply_blurpool(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08242757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ResNet50 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmkuznecov/miniconda3/envs/ffcv_mri/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying BlurPool...\n"
     ]
    }
   ],
   "source": [
    "nesting_start = 3\n",
    "efficient = True\n",
    "nesting_list = [2**i for i in range(nesting_start, 12)]  # 8, 16, 32, 64, 128, 256, 512, 1024, 2048\n",
    "num_classes = 1000\n",
    "model_path = \"/home/mmkuznecov/SkolCourses/DL/FINAL_PROJECT/MRL/train/logs/98819cd7-62aa-479e-8642-f4333540615e/final_weights.pt\"  # Update with your actual path\n",
    "\n",
    "# Create the model\n",
    "print(\"Initializing ResNet50 model...\")\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.fc = MRL_Linear_Layer(nesting_list, num_classes=num_classes, efficient=efficient)\n",
    "\n",
    "# Apply BlurPool to the model\n",
    "print(\"Applying BlurPool...\")\n",
    "apply_blurpool(model)\n",
    "\n",
    "# Convert to channels-last memory format for better performance\n",
    "model = model.to(memory_format=torch.channels_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c272271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): BlurPoolConv2d(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): BlurPoolConv2d(\n",
       "          (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): BlurPoolConv2d(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): BlurPoolConv2d(\n",
       "          (conv): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): BlurPoolConv2d(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): BlurPoolConv2d(\n",
       "          (conv): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): MRL_Linear_Layer(\n",
       "    (nesting_classifier_0): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1af5c3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating random input tensor...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nGenerating random input tensor...\")\n",
    "batch_size = 4\n",
    "random_input = torch.randn(batch_size, 3, 224, 224, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbc28d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_input = random_input.to(memory_format=torch.channels_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d890e87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing forward pass...\n",
      "\n",
      "Output shapes from all embedding dimensions:\n",
      "Embedding Dim   Output Shape         Parameters\n",
      "--------------------------------------------------\n",
      "8               torch.Size([4, 1000]) 8,000\n",
      "16              torch.Size([4, 1000]) 16,000\n",
      "32              torch.Size([4, 1000]) 32,000\n",
      "64              torch.Size([4, 1000]) 64,000\n",
      "128             torch.Size([4, 1000]) 128,000\n",
      "256             torch.Size([4, 1000]) 256,000\n",
      "512             torch.Size([4, 1000]) 512,000\n",
      "1024            torch.Size([4, 1000]) 1,024,000\n",
      "2048            torch.Size([4, 1000]) 2,048,000\n",
      "--------------------------------------------------\n",
      "Total classifier parameters (MRL-E): 2,048,000\n",
      "\n",
      "Example output values (first 5) from embedding dim 8:\n",
      "tensor([-0.0172, -0.0201,  0.0015,  0.0222, -0.0080], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"Performing forward pass...\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(random_input)\n",
    "\n",
    "# Print the shapes of all outputs\n",
    "print(\"\\nOutput shapes from all embedding dimensions:\")\n",
    "print(f\"{'Embedding Dim':<15} {'Output Shape':<20} {'Parameters'}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "total_params = 0\n",
    "for i, dim in enumerate(nesting_list):\n",
    "    logits = outputs[i]\n",
    "    num_params = dim * num_classes\n",
    "    if efficient:\n",
    "        # In efficient mode, we count parameters up to this dimension\n",
    "        if i == len(nesting_list) - 1:\n",
    "            params_to_add = num_params\n",
    "        else:\n",
    "            params_to_add = 0\n",
    "    else:\n",
    "        # In non-efficient mode, each dimension has its own parameters\n",
    "        params_to_add = num_params\n",
    "        \n",
    "    total_params += params_to_add\n",
    "    print(f\"{dim:<15} {str(logits.shape):<20} {num_params:,}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "if efficient:\n",
    "    print(f\"Total classifier parameters (MRL-E): {nesting_list[-1] * num_classes:,}\")\n",
    "else:\n",
    "    print(f\"Total classifier parameters (MRL): {total_params:,}\")\n",
    "\n",
    "# Output example values from the first embedding\n",
    "print(f\"\\nExample output values (first 5) from embedding dim {nesting_list[0]}:\")\n",
    "print(outputs[0][0, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1bcfd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmkuznecov/miniconda3/envs/ffcv_mri/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ResNet50 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmkuznecov/miniconda3/envs/ffcv_mri/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/mmkuznecov/miniconda3/envs/ffcv_mri/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/tmp/ipykernel_2696696/2298902541.py:94: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying BlurPool...\n",
      "Loading weights from /home/mmkuznecov/SkolCourses/DL/FINAL_PROJECT/MRL/train/logs/98819cd7-62aa-479e-8642-f4333540615e/final_weights.pt...\n",
      "Model loaded successfully!\n",
      "Loading dataset from data/imagenet_1k_resized_256_val...\n",
      "Dataset loaded with 50000 samples\n",
      "Processing 50000 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [02:45<00:00,  4.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation completed on 50000 images\n",
      "\n",
      "Accuracy results for each embedding dimension:\n",
      "Embedding Dim   Top-1 Accuracy (%)   Top-5 Accuracy (%)\n",
      "------------------------------------------------------------\n",
      "8               53.55                77.07               \n",
      "16              67.61                85.14               \n",
      "32              70.27                87.76               \n",
      "64              71.12                88.80               \n",
      "128             71.55                89.61               \n",
      "256             71.85                90.15               \n",
      "512             72.02                90.33               \n",
      "1024            72.11                90.43               \n",
      "2048            72.15                90.55               \n",
      "\n",
      "Best performing dimension: 2048\n",
      "Top-1 Accuracy: 72.15%\n",
      "Top-5 Accuracy: 90.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasets import load_from_disk\n",
    "from PIL import Image\n",
    "\n",
    "# Define the MRL components from the code base\n",
    "class MRL_Linear_Layer(nn.Module):\n",
    "    def __init__(self, nesting_list, num_classes=1000, efficient=False, **kwargs):\n",
    "        super(MRL_Linear_Layer, self).__init__()\n",
    "        self.nesting_list = nesting_list\n",
    "        self.num_classes = num_classes\n",
    "        self.efficient = efficient\n",
    "        \n",
    "        if self.efficient:\n",
    "            setattr(self, f\"nesting_classifier_{0}\", nn.Linear(nesting_list[-1], self.num_classes, **kwargs))\n",
    "        else:    \n",
    "            for i, num_feat in enumerate(self.nesting_list):\n",
    "                setattr(self, f\"nesting_classifier_{i}\", nn.Linear(num_feat, self.num_classes, **kwargs))    \n",
    "\n",
    "    def reset_parameters(self):\n",
    "        if self.efficient:\n",
    "            self.nesting_classifier_0.reset_parameters()\n",
    "        else:\n",
    "            for i in range(len(self.nesting_list)):\n",
    "                getattr(self, f\"nesting_classifier_{i}\").reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        nesting_logits = ()\n",
    "        for i, num_feat in enumerate(self.nesting_list):\n",
    "            if self.efficient:\n",
    "                if self.nesting_classifier_0.bias is None:\n",
    "                    nesting_logits += (torch.matmul(x[:, :num_feat], (self.nesting_classifier_0.weight[:, :num_feat]).t()), )\n",
    "                else:\n",
    "                    nesting_logits += (torch.matmul(x[:, :num_feat], (self.nesting_classifier_0.weight[:, :num_feat]).t()) + self.nesting_classifier_0.bias, )\n",
    "            else:\n",
    "                nesting_logits += (getattr(self, f\"nesting_classifier_{i}\")(x[:, :num_feat]),)\n",
    "\n",
    "        return nesting_logits\n",
    "\n",
    "class FixedFeatureLayer(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, **kwargs):\n",
    "        super(FixedFeatureLayer, self).__init__(in_features, out_features, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not (self.bias is None):\n",
    "            out = torch.matmul(x[:, :self.in_features], self.weight.t()) + self.bias\n",
    "        else:\n",
    "            out = torch.matmul(x[:, :self.in_features], self.weight.t())\n",
    "        return out\n",
    "\n",
    "class BlurPoolConv2d(nn.Module):\n",
    "    def __init__(self, conv):\n",
    "        super().__init__()\n",
    "        default_filter = torch.tensor([[[[1, 2, 1], [2, 4, 2], [1, 2, 1]]]]) / 16.0\n",
    "        filt = default_filter.repeat(conv.in_channels, 1, 1, 1)\n",
    "        self.conv = conv\n",
    "        self.register_buffer('blur_filter', filt)\n",
    "\n",
    "    def forward(self, x):\n",
    "        blurred = F.conv2d(x, self.blur_filter, stride=1, padding=(1, 1),\n",
    "                           groups=self.conv.in_channels, bias=None)\n",
    "        return self.conv.forward(blurred)\n",
    "\n",
    "def apply_blurpool(mod: nn.Module):\n",
    "    for (name, child) in mod.named_children():\n",
    "        if isinstance(child, nn.Conv2d) and (np.max(child.stride) > 1 and child.in_channels >= 16):\n",
    "            setattr(mod, name, BlurPoolConv2d(child))\n",
    "        else: \n",
    "            apply_blurpool(child)\n",
    "\n",
    "def load_model(model_path):\n",
    "    # Model configuration\n",
    "    nesting_start = 3\n",
    "    efficient = True\n",
    "    nesting_list = [2**i for i in range(nesting_start, 12)]  # 8, 16, 32, 64, 128, 256, 512, 1024, 2048\n",
    "    num_classes = 1000\n",
    "    \n",
    "    # Create the model\n",
    "    print(\"Initializing ResNet50 model...\")\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    model.fc = MRL_Linear_Layer(nesting_list, num_classes=num_classes, efficient=efficient)\n",
    "    \n",
    "    # Apply BlurPool to the model\n",
    "    print(\"Applying BlurPool...\")\n",
    "    apply_blurpool(model)\n",
    "    \n",
    "    # Load the pretrained weights\n",
    "    try:\n",
    "        print(f\"Loading weights from {model_path}...\")\n",
    "        checkpoint = torch.load(model_path, map_location='cpu')\n",
    "        \n",
    "        # Handle case where weights were saved in DataParallel format\n",
    "        if list(checkpoint.keys())[0].startswith('module.'):\n",
    "            # Remove 'module.' prefix\n",
    "            clean_ckpt = {}\n",
    "            for k, v in checkpoint.items():\n",
    "                clean_ckpt[k[7:] if k.startswith('module.') else k] = v\n",
    "            checkpoint = clean_ckpt\n",
    "        \n",
    "        model.load_state_dict(checkpoint)\n",
    "        print(\"Model loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading weights: {e}\")\n",
    "        print(\"Continuing with pretrained weights only.\")\n",
    "    \n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model = model.to(memory_format=torch.channels_last)\n",
    "    model.eval()\n",
    "    \n",
    "    return model, nesting_list, device\n",
    "\n",
    "def evaluate_dataset(model, dataset, nesting_list, device, batch_size=64, num_samples=None):\n",
    "    # Standard ImageNet normalization\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "    \n",
    "    # Create transformation pipeline\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "    \n",
    "    # Initialize accuracy trackers for each embedding dimension\n",
    "    correct_top1 = {dim: 0 for dim in nesting_list}\n",
    "    correct_top5 = {dim: 0 for dim in nesting_list}\n",
    "    \n",
    "    # Limit samples if specified\n",
    "    if num_samples is not None:\n",
    "        total_samples = min(num_samples, len(dataset))\n",
    "    else:\n",
    "        total_samples = len(dataset)\n",
    "    \n",
    "    # Process in batches\n",
    "    batches = []\n",
    "    labels = []\n",
    "    \n",
    "    print(f\"Processing {total_samples} images...\")\n",
    "    for i in tqdm(range(0, total_samples, batch_size)):\n",
    "        batch_indices = range(i, min(i + batch_size, total_samples))\n",
    "        batch_images = []\n",
    "        batch_labels = []\n",
    "        \n",
    "        # Process each image in the batch\n",
    "        for idx in batch_indices:\n",
    "            sample = dataset[idx]\n",
    "            image = sample['image']\n",
    "            label = sample['label']\n",
    "            \n",
    "            # Apply transformations\n",
    "            img_tensor = transform(image)\n",
    "            batch_images.append(img_tensor)\n",
    "            batch_labels.append(label)\n",
    "        \n",
    "        # Stack into a batch tensor\n",
    "        images_tensor = torch.stack(batch_images).to(device)\n",
    "        labels_tensor = torch.tensor(batch_labels).to(device)\n",
    "        \n",
    "        # Convert to channels last format for better performance\n",
    "        images_tensor = images_tensor.to(memory_format=torch.channels_last)\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images_tensor)\n",
    "        \n",
    "        # Calculate accuracy for each embedding dimension\n",
    "        for i, dim in enumerate(nesting_list):\n",
    "            # Top-1 accuracy\n",
    "            _, predicted = outputs[i].max(1)\n",
    "            correct_top1[dim] += (predicted == labels_tensor).sum().item()\n",
    "            \n",
    "            # Top-5 accuracy\n",
    "            _, top5_indices = outputs[i].topk(5, dim=1)\n",
    "            labels_expanded = labels_tensor.view(-1, 1).expand_as(top5_indices)\n",
    "            correct_top5[dim] += (top5_indices == labels_expanded).sum().item()\n",
    "    \n",
    "    # Calculate final accuracy for each dimension\n",
    "    accuracy_top1 = {dim: correct_top1[dim] / total_samples * 100 for dim in nesting_list}\n",
    "    accuracy_top5 = {dim: correct_top5[dim] / total_samples * 100 for dim in nesting_list}\n",
    "    \n",
    "    return accuracy_top1, accuracy_top5, total_samples\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    model_path = \"/home/mmkuznecov/SkolCourses/DL/FINAL_PROJECT/MRL/train/logs/98819cd7-62aa-479e-8642-f4333540615e/final_weights.pt\"\n",
    "    dataset_path = \"data/imagenet_1k_resized_256_val\"\n",
    "    batch_size = 64  # Adjust based on your GPU memory\n",
    "    num_samples = None  # Set to None to process all images, or a number to limit samples\n",
    "    \n",
    "    # Load model\n",
    "    model, nesting_list, device = load_model(model_path)\n",
    "    \n",
    "    # Load dataset\n",
    "    print(f\"Loading dataset from {dataset_path}...\")\n",
    "    dataset = load_from_disk(dataset_path)\n",
    "    print(f\"Dataset loaded with {len(dataset)} samples\")\n",
    "    \n",
    "    # Evaluate the model on the dataset\n",
    "    accuracy_top1, accuracy_top5, total_samples = evaluate_dataset(\n",
    "        model, dataset, nesting_list, device, batch_size, num_samples\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nEvaluation completed on {total_samples} images\")\n",
    "    print(\"\\nAccuracy results for each embedding dimension:\")\n",
    "    print(f\"{'Embedding Dim':<15} {'Top-1 Accuracy (%)':<20} {'Top-5 Accuracy (%)'}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for dim in nesting_list:\n",
    "        print(f\"{dim:<15} {accuracy_top1[dim]:<20.2f} {accuracy_top5[dim]:<20.2f}\")\n",
    "    \n",
    "    # Print a summary for the best dimension\n",
    "    best_dim = max(accuracy_top1, key=accuracy_top1.get)\n",
    "    print(f\"\\nBest performing dimension: {best_dim}\")\n",
    "    print(f\"Top-1 Accuracy: {accuracy_top1[best_dim]:.2f}%\")\n",
    "    print(f\"Top-5 Accuracy: {accuracy_top5[best_dim]:.2f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ffcv_mri",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
